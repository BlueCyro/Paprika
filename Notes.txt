# Paprika restructure

Up until this point, I've been getting familiar with the methodology behind rasterizing triangles and need to restructure how things are done to provide a better framework


# What to restructure?

Pixel pusher is mostly okay, but it needs a few more things.

* Shader objects, how to define?
    * Interface-implementing structs?
        + Provides unique code paths for each shader object
        + Allows self-containment for varying types of rendering behavior
        * How is rendering behavior defined?
            * Currently, rasterizer chooses pixels and performs inline behavior to set them
            * Instead of inline behavior, triangle pixels should be passed to shader objects
                * How to define pixel choosing behavior? Vertex shader?
                    * Make vertex shader pre-process incoming triangles to prepare for raster
                    + Vertex shader performs necessary transformations, can attach attributes to triangles
                    * How to define attributes???
                        + Vertex shaders implement a default "IVertexShader" interface to mark them as such
                        + Can implement multiple interfaces which define further attributes
                        ++ Later-pipeline shaders can take potentially any IVertexShader, but only use attributes that are available
                            -- This requires testing the interface, which may not be implemented. Potential wasted work per triangle, will add up with potential millions.
                                * Does this actually matter? Maybe not. Usually vertex shaders are made for the fragment shader(s) they're targetting.
                                @ Exploration is required, will test this first.

                + Shader objects define callbacks that are executed when they receive a triangle pixel
                * What to pass to pixel callback? Data object? What form?
                    * Should probably be some form of base interface implementation, still struct (potentialy ref struct for lightness?)
                        - Ref struct removes interfaces (does this matter still? probably not)
                        * Normal structs could also be allocated on the stack, but could leak to heap
                    * What if it wasn't an interface and was instead a generic ref struct that is typed with another struct?
                        + Provides a standardized way to pack data to the fragment shader
                        + Fragment shader can then unpack payload with generic function defined on ref struct
                        + Reinterpretation of span as T guarantees payload to be on stack - can be put on heap when required
                        + T can then be an interfaced struct if need-be for further standardization
                        --- Spans cannot be overlaid on ref structs, but that's actually okay. The ref struct is just meant as a transfer medium



# How to register geometry with rendering context?

* Extensible mesh class? What do we even put on it?
    * What base features would be required to render a model?
        = Triangles, required
            * Although maybe not since point clouds exist? Maybe we just don't care and handle these in a vertex shader instead
        = UVs of each triangle point
        = Material index of each triangle
            ! We'll need to define materials at some point
        = Maybe more properties as I think of them, adding properties should be easy with this pipeline




# It's rewind time (breakdown of high-level idea)

1) Mesh geometry is read from disk and stored in a mesh class
2) Mesh class describes world transform of model
3) Vertex shader reads mesh data, decides what attributes to take, packages data into custom struct implementation defined by interface, used as generic type in ref struct carrier
4) Package is post-processed by rasterizer to transform points into screen coordinates and interpolate all lerpable values
5) Fragment shader receives per-pixel interpolated data, decides what color the pixel is based on this data


# Where do we start?

- Probably restructure the pixel pusher loop a little bit and move a few things to be internal.
- Remove event-based code execution, start making mesh class. Figure out what vertex shader needs for interface.
    * Mesh class can be big and slow, upload will handle boiling down everything into efficient data structures
    * What does a vertex have on it?
        - Position
        - UV
        - Normal
        - Attributes
            * Attributes are defined as one or more primitive types
            * Attributes are interpolated
                ! Only floating point types!!!
            * Attributes can optionally be constant; non-interpolated
        - Vertex type
            * Keeping it simple: Either point or triangle, assumed triangle unless queried.
        - Material index
    * Vertices are stored in slow mesh format
        - Vertex data is sorted by material index
        - Vertex shader reads incoming vertices
        - Attributes are read or added depending on type of vertex shader
        - Attributes are stored in user-defined attribute struct
        - Attributes are then passed via ref struct container to rasterizer for fragmenting
    
    * Triangle is generated from vertices.
    * What does a triangle have on it?
        - 3 vertices
        - Implicit normal
        - Edges
        - Edge function bases
        ? Maybe make it generic for 2D and 3D?



# How should a camera be defined?

* Defines view and projection matrices
    - Resolution, FOV, Position, Rotation
* What else should it define?
    * Perhaps instead leave resolution to render buffer?
        - Render buffer is just a big array of value types that hold pixels
        - Dumb object, doesn't need lots of complexity
        - Could literally just be generic struct that defines width, height, and array of given value type
    * Generic? Should it define a rendering context by which to render in?
        + This could mean that contexts are modular
        + Rendering contexts (different rasterizers, stylized, raytracers, etc.) could be defined per-camera
        ? What is a context?
            * Rendering context defines method by which the scene is rendered
            * Does not contain scene itself, only changes how it is viewed