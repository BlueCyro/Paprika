# Paprika restructure

Up until this point, I've been getting familiar with the methodology behind rasterizing triangles and need to restructure how things are done to provide a better framework


# What to restructure?

Pixel pusher is mostly okay, but it needs a few more things.

* Shader objects, how to define?
    * Interface-implementing structs?
        + Provides unique code paths for each shader object
        + Allows self-containment for varying types of rendering behavior
        * How is rendering behavior defined?
            * Currently, rasterizer chooses pixels and performs inline behavior to set them
            * Instead of inline behavior, triangle pixels should be passed to shader objects
                * How to define pixel choosing behavior? Vertex shader?
                    * Make vertex shader pre-process incoming triangles to prepare for raster
                    + Vertex shader performs necessary transformations, can attach attributes to triangles
                    * How to define attributes???
                        + Vertex shaders implement a default "IVertexShader" interface to mark them as such
                        + Can implement multiple interfaces which define further attributes
                        ++ Later-pipeline shaders can take potentially any IVertexShader, but only use attributes that are available
                            -- This requires testing the interface, which may not be implemented. Potential wasted work per triangle, will add up with potential millions.
                                * Does this actually matter? Maybe not. Usually vertex shaders are made for the fragment shader(s) they're targetting.
                                @ Exploration is required, will test this first.

                + Shader objects define callbacks that are executed when they receive a triangle pixel
                * What to pass to pixel callback? Data object? What form?
                    * Should probably be some form of base interface implementation, still struct (potentialy ref struct for lightness?)
                        - Ref struct removes interfaces (does this matter still? probably not)
                        * Normal structs could also be allocated on the stack, but could leak to heap
                    * What if it wasn't an interface and was instead a generic ref struct that is typed with another struct?
                        + Provides a standardized way to pack data to the fragment shader
                        + Fragment shader can then unpack payload with generic function defined on ref struct
                        + Reinterpretation of span as T guarantees payload to be on stack - can be put on heap when required
                        + T can then be an interfaced struct if need-be for further standardization
                        --- Spans cannot be overlaid on ref structs, but that's actually okay. The ref struct is just meant as a transfer medium



# How to register geometry with rendering context?

* Extensible mesh class? What do we even put on it?
    * What base features would be required to render a model?
        = Triangles, required
            * Although maybe not since point clouds exist? Maybe we just don't care and handle these in a vertex shader instead
        = UVs of each triangle point
        = Material index of each triangle
            ! We'll need to define materials at some point
        = Maybe more properties as I think of them, adding properties should be easy with this pipeline




# It's rewind time (breakdown of high-level idea)

1) Mesh geometry is read from disk and stored in a mesh class
2) Mesh class describes world transform of model
3) Vertex shader reads mesh data, decides what attributes to take, packages data into custom struct implementation defined by interface, used as generic type in ref struct carrier
4) Rasterizer processes 
4) Package is post-processed by rasterizer to transform points into screen coordinates and interpolate all lerpable values
5)
